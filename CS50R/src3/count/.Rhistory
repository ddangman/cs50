set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
f = qf(alpha, df1 = p, df2 = n1+n2-p)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = n1+n2-2, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
# get large sample confidence interval
get_simultaneous_confidence_interval()
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = p, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
#me = sqrt(f * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = p, df2 = n1+n2-p-1)
me = sqrt(p * (n1+n2-2) * f / (n1+n2-p-1) * ((variance1/n1) + (variance2/n2)))
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get large sample confidence interval
get_simultaneous_confidence_interval()
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = p, df2 = n1+n2-p-1)
c <- sqrt( p * (n1+n2-2) * f / (n1+n2-p-1) )
me = c * sqrt( (variance1/n1) + (variance2/n2) )
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = p, df2 = n1+n2-p-1)
c <- sqrt( p * (n1+n2-2) * f / (n1+n2-p-1) )
me = c * sqrt( (variance1/n1) + (variance2/n2) )
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get small sample confidence interval
get_simultaneous_confidence_interval(TRUE)
library(ICSNP)
## Testing for Equality of Mean Vectors when Σ1 ≠ Σ2
HotellingsT2(real_swiss,fake_swiss)
get_hotelling <- function(test) {
f <- unlist(test$statistic)
n <- unlist(sum(test$parameter))
p <- test$parameter[1]
hotelling <- f * p * (n-1) / (n-p)
return(hotelling)
}
nutrient<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\nutrient.csv",header=TRUE)
# null hypothesis amount is recommended intake
calcium0 <- 1000
iron0 <- 15
protein0 <- 60
vitaminA0 <- 800
vitaminC0 <- 75
mu0 <- c(calcium0, iron0, protein0, vitaminA0, vitaminC0)
### Hotelling’s T-square
# https://online.stat.psu.edu/stat505/lesson/7/7.1/7.1.4
x <- nutrient[c("calcium", "iron", "protein", "vitamin.A", "vitamin.C")]
test=HotellingsT2(x,mu=mu0,test="f")
print(test)
## Testing for Equality of Mean Vectors when Σ1 ≠ Σ2
# drop 'type' column
drops <- c("type")
real_swiss_x = real_swiss[ , !(names(real_swiss) %in% drops)]
fake_swiss_y = fake_swiss[ , !(names(fake_swiss) %in% drops)]
HotellingsT2(real_swiss_x,fake_swiss_y)
## MANOVA
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
View(pottery)
View(pottery)
View(pottery)
View(pottery)
pottery2 <- pottery %>%
select(al, fe, mg, ca, na) %>%
add_column(id=1:nrow(pottery), .before=1)
library(dplyr)
pottery2 <- pottery %>%
select(al, fe, mg, ca, na) %>%
add_column(id=1:nrow(pottery), .before=1)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(car)
library(broom)
pottery2 <- pottery %>%
select(al, fe, mg, ca, na) %>%
add_column(id=1:nrow(pottery), .before=1)
ggboxplot(
pottery2, x = "site", y = c("al", "fe", "mg", "ca", "na"),
merge = TRUE, palette = "jco"
)
ggboxplot(
pottery, x = "site", y = c("al", "fe", "mg", "ca", "na"),
merge = TRUE, palette = "jco"
)
## Box's Test ##
library(heplots)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
boxM(cbind(al, fe, mg, ca, na) ~ site, data=pottery)
box_m(pottery[, -5], pottery[, 5])
pottery[, -5]
pottery[, -4]
pottery[, -1]
box_m(pottery[, -1], pottery[, 1])
pottery[, 1]
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
box_m(pottery[, -1], pottery[, 1])
## Box's Test ##
library(heplots)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
box_m(pottery[, -1], pottery[, 1])
# Box's M-test for Homogeneity of Covariance Matrices
library(rstatix)
box_m(pottery[, -1], pottery[, 1])
View(pottery)
View(pottery)
# Box's M-test for Homogeneity of Covariance Matrices
library(rstatix)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
box_m(pottery[, -1], pottery[, 1])
View(pottery)
View(pottery)
## orthogonal contrast
a_site <- subset(pottery, site=="A")
c_site <- subset(pottery, site=="C")
i_site <- subset(pottery, site=="I")
l_site <- subset(pottery, site=="L")
a1, c1, i1, l1 <- 1, 2, 3, 4
na <- length(a_site)
na <- dim(a_site)
na <- dim(a_site)[1]
a_site <- subset(pottery, site=="A")
c_site <- subset(pottery, site=="C")
i_site <- subset(pottery, site=="I")
l_site <- subset(pottery, site=="L")
a1 <- 8/16
c1 <- -2/16
i1 <- 8/16
l1 <- -14/16
a2 <- 1
c2 <- 0
i2 <- -1
l2 <- 0
a3 <- 0
c3 <- 1
i3 <- 0
l3 <- -1
na <- dim(a_site)[1]
nc <- dim(c_site)[1]
ni <- dim(i_site)[1]
nl <- dim(l_site)[1]
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
# box_m(pottery[, -1], pottery[, 1])
## orthogonal contrast
a_site <- subset(pottery, site=="A")
c_site <- subset(pottery, site=="C")
i_site <- subset(pottery, site=="I")
l_site <- subset(pottery, site=="L")
na <- dim(a_site)[1]
nc <- dim(c_site)[1]
ni <- dim(i_site)[1]
nl <- dim(l_site)[1]
coeff1 <- c(8/16,-2/16,8/16,-14/16)
coeff2 <- c(1,0,-1,0)
coeff3 <- c(0,1,0,-1)
coeff1 <- c(8/16,-2/16,8/16,-14/16)
coeff2 <- c(1/2,0,-1/2,0)
coeff3 <- c(0,1/2,0,-1/2)
library(dplyr)
swiss<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\swiss3.csv",header=TRUE)
real_swiss <- subset(swiss,type=="real")
fake_swiss <- subset(swiss,type=="fake")
# reset indices
p <- 6
alpha <- 0.05
n1 <- nrow(real_swiss)
n2 <- nrow(fake_swiss)
chi2 <- qchisq(p = alpha, df = p, lower.tail = FALSE)
get_simultaneous_confidence_interval <- function(isSmallSample=FALSE){
for (i in 2:7){
set1 <- real_swiss %>% select(all_of(i))
set2 <- fake_swiss %>% select(all_of(i))
mu1 <- mean(set1[,1])
mu2 <- mean(set2[,1])
variance1 <- sd(set1[,1])**2
variance2 <- sd(set2[,1])**2
me <- sqrt(chi2 * ((variance1/n1) + (variance2/n2)))
if (isSmallSample){
f <- qf(alpha, df1 = p, df2 = n1+n2-p-1)
c <- sqrt( p * (n1+n2-2) * f / (n1+n2-p-1) )
me = c * sqrt( (variance1/n1) + (variance2/n2) )
}
diff <- mu1 - mu2
lb <- diff-me
ub <- diff+me
print(paste(colnames(set1), lb, ub))
}
}
# get large sample confidence interval
get_simultaneous_confidence_interval()
coeff1[1]
library(dplyr)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
a_site <- subset(pottery, site=="A")
c_site <- subset(pottery, site=="C")
i_site <- subset(pottery, site=="I")
l_site <- subset(pottery, site=="L")
na <- dim(a_site)[1]
nc <- dim(c_site)[1]
ni <- dim(i_site)[1]
nl <- dim(l_site)[1]
coeff1 <- c(8/16,-2/16,8/16,-14/16)
coeff2 <- c(1/2,0,-1/2,0)
coeff3 <- c(0,1/2,0,-1/2)
get_contrasts <- function(coeff){
for (i in 2:6){
ai <- a_site %>% select(all_of(i))
ci <- c_site %>% select(all_of(i))
ii <- i_site %>% select(all_of(i))
li <- l_site %>% select(all_of(i))
est <- coeff[1] * mean(ai[,1]) + coeff[2] * mean(ci[,1]) + coeff[3] * mean(ii[,1]) + coeff[4] * mean(li[,1])
print(paste(colnames(set1), est))
}
}
get_contrasts(coeff1)
get_contrasts <- function(coeff){
for (i in 2:6){
ai <- a_site %>% select(all_of(i))
ci <- c_site %>% select(all_of(i))
ii <- i_site %>% select(all_of(i))
li <- l_site %>% select(all_of(i))
est <- coeff[1] * mean(ai[,1]) + coeff[2] * mean(ci[,1]) + coeff[3] * mean(ii[,1]) + coeff[4] * mean(li[,1])
print(paste(colnames(ai), est))
}
}
get_contrasts(coeff1)
get_contrasts(coeff2)
get_contrasts(coeff3)
coeff2 <- c(1,0,-1,0)
get_contrasts(coeff2)
coeff3 <- c(0,1,0,-1)
get_contrasts(coeff3)
library(dplyr)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
a_site <- subset(pottery, site=="A")
c_site <- subset(pottery, site=="C")
i_site <- subset(pottery, site=="I")
l_site <- subset(pottery, site=="L")
na <- dim(a_site)[1]
nc <- dim(c_site)[1]
ni <- dim(i_site)[1]
nl <- dim(l_site)[1]
coeff1 <- c(8/16,-2/16,8/16,-14/16)
coeff2 <- c(1,0,-1,0)
coeff3 <- c(0,1,0,-1)
get_contrasts <- function(coeff){
for (i in 2:6){
ai <- a_site %>% select(all_of(i))
ci <- c_site %>% select(all_of(i))
ii <- i_site %>% select(all_of(i))
li <- l_site %>% select(all_of(i))
est <- coeff[1] * mean(ai[,1]) + coeff[2] * mean(ci[,1]) + coeff[3] * mean(ii[,1]) + coeff[4] * mean(li[,1])
print(paste(colnames(ai), est))
}
}
get_contrasts(coeff1)
get_contrasts(coeff2)
get_contrasts(coeff3)
pottery<-read.csv("E:\\Documents2TBm2\\MiniTab\\PennState\\pottery.csv",header=TRUE)
pottery$site <- as.factor(pottery$site)
pottery.manova <- manova(cbind(pottery$al, pottery$fe, pottery$mg, pottery$ca, pottery$na) ~ pottery.site, data = pottery)
pottery.manova <- manova(cbind(pottery$al, pottery$fe, pottery$mg, pottery$ca, pottery$na) ~ pottery$site, data = pottery)
pottery.manova <- manova(cbind(pottery$al, pottery$fe, pottery$mg, pottery$ca, pottery$na) ~ pottery$site, data = pottery)
pottery.summary <- summary(pottery.manova)
pottery.summary
pottery.summary <- summary(pottery.manova, test='Wilks')
pottery.summary
get_stationary_distribution(p23)
get_stationary_distribution <- function(mc) {
n <- NROW(mc)
tr <- t(mc)
print("Transpose is")
print(tr)
pti <- tr - diag(n)
print("Transpose subtract identity is")
print(pti)
for(c in 1:n) {
pti[n, c] = 1
}
print("all 1 bottom row")
print(pti)
print("inverse is")
ipti <- solve(pti)
print(ipti)
print("right column is eigenvector")
print(ipti[,n])
}
names23 <- c("Sun", "Rain")
m23 <- c(.8, .2,
.3, .7)
get_stationary_distribution(p23)
p23 <- matrix(data = m23,
nrow = 2,
byrow = TRUE,
dimnames = list(names23, names23))
get_stationary_distribution(p23)
print("hello, world")
print("hello, world")
load("E:/Documents2TBm2/StatisticDocuments/Harvard/CS50R/src2/outliers/temps.RData")
mean(temps)
temps
which(temps < 0)
all(temps < 0 | temps > 60)
all(temps < 100)
temps[-which(temps < 0 | temps > 60)]
temps[which(temps < 0 | temps > 60)]
filter <- temps < 0 | temps > 60
temps[filter]
temps[temps < 0 | temps > 60]
temps[filter]
temps[which(temps < 0 | temps > 60)]
temps[temps < 0 | temps > 60]
filter <- temps < 0 | temps > 60
temps[filter]
load("E:/Documents2TBm2/StatisticDocuments/Harvard/CS50R/src2/outliers/no_outliers.RData")
load("E:/Documents2TBm2/StatisticDocuments/Harvard/CS50R/src2/outliers/outliers.RData")
setwd("E:/Documents2TBm2/StatisticDocuments/Harvard/CS50R/src3/count")
